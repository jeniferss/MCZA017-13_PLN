{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeniferss/MCZA017-13_PLN/blob/main/2025_Q3_PLN_PROJETO_PR%C3%81TICO_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **PROJETO PRÁTICO** [LangChain + Grandes Modelos de Linguagem]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "O **PROJETO PRÁTICO** deve ser feito utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook armazenado no Google Drive e o link de um repositório no GitHub devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/D4gLqP1iGgyn2hbH8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia **07/12 (domingo)** APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`JENIFER SOARES SOUZA           11202020219`\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "`JUAN PABLO MORENO OLIVEIRA     11202131494`"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VbYD2mw8y4CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada equipe deve selecionar um Grande Modelo de Linguagem (*Large Language Model - LMM*).\n",
        "\n"
      ],
      "metadata": {
        "id": "_UlblxFxzDV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados do LLM selecionada:\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**LLM**: GPT-4.1 mini ou gemini-2.5-flash\n",
        "\n",
        ">\n",
        "\n",
        "**Link para a documentação oficial**: https://openai.com/index/gpt-4-1/ ou https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash?hl=pt-br\n",
        "\n"
      ],
      "metadata": {
        "id": "a6AkE6iW0c3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API (Opcional)**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados da API selecionada:\n",
        "\n",
        "**API**:Gutendex\n",
        "\n",
        "**Site oficial**:https://gutendex.com/\n",
        "\n",
        "**Link para a documentação oficial**:https://gutendex.com/ (na homepage)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso do framework **`LangChain`** (obrigatório) e de um **LLM** aplicando, no mínimo, DUAS técnicas de PLN. As técnicas podem ser aplicada em qualquer córpus obtido a partir de uma **API** ou a partir de uma página Web.\n",
        "\n",
        "O **LLM** e a **API** selecionados devem ser informados na seguinte planilha:\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1iIUZcwnywO7RuF6VEJ8Rx9NDT1cwteyvsnkhYr0NWtU/edit?usp=sharing\n",
        "\n",
        ">\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   Similaridade de Textos\n",
        "*   Reconhecimento de Entidades Nomeadas\n",
        "*   Sistemas de Perguntas e Respostas\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação os seguintes pontos:\n",
        "\n",
        "* Uso do framework **`LangChain`**.\n",
        "\n",
        "* Escolha e uso de um **LLM**.\n",
        "\n",
        "* Escolha e uso de uma **API** ou **Página Web**.\n",
        "\n",
        "* Projeto disponível no Github.\n",
        "\n",
        "* Apresentação (5 a 10 minutos).\n",
        "\n",
        "* Criatividade no uso do framework **`LangChain`** em conjunto com o **LLM** e a **API**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo o código do notebook deve ser executado. Código sem execução não será considerado."
      ],
      "metadata": {
        "id": "LhwdrMp123Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste trabalho, utilizamos como fonte de dados o [Projeto Gutenberg](https://www.gutenberg.org/about/), que disponibiliza obras literárias em domínio público. A partir desse acervo, selecionamos quatro livros brasileiros de interesse para análise — *Dom Casmurro*, *O Cortiço* e *Quincas Borba*. Para integrá-los ao pipeline, realizamos o mapeamento manual de seus respectivos *IDs* no catálogo e utilizamos a API [Gutendex](https://gutendex.com/) para recuperar metadados essenciais, incluindo as URLs que fornecem o texto completo em formato **.txt**."
      ],
      "metadata": {
        "id": "GLEf_mkP8APk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalação de bibliotecas"
      ],
      "metadata": {
        "id": "u5BMqeVzEw98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"langchain[google-genai]\""
      ],
      "metadata": {
        "id": "mtuteUpoeSgH",
        "outputId": "e0a48877-785e-4e7f-e629-eb375dec0df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain[google-genai] in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from langchain[google-genai]) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain[google-genai]) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain[google-genai]) (2.11.10)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (from langchain[google-genai]) (3.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (0.4.43)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.4.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai->langchain[google-genai]) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai->langchain[google-genai]) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (5.29.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (0.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain[google-genai]) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.3.1)\n"
          ]
        }
      ],
      "execution_count": 99
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importação de bibliotecas"
      ],
      "metadata": {
        "id": "z6ADfHeKE05j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import spacy\n",
        "import unicodedata\n",
        "from google.colab import userdata\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from pydantic import BaseModel, HttpUrl\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as EN_STOPWORDS\n",
        "from time import sleep\n",
        "from typing import List, Optional, Dict, Set"
      ],
      "metadata": {
        "id": "wnBtlMxq6e1t"
      },
      "outputs": [],
      "execution_count": 100
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definição dos modelos de dados"
      ],
      "metadata": {
        "id": "tawuPyk8FIOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person(BaseModel):\n",
        "    name: str\n",
        "    birth_year: Optional[int] = None\n",
        "    death_year: Optional[int] = None\n",
        "\n",
        "\n",
        "class BookMetadataInput(BaseModel):\n",
        "    id: int\n",
        "    title: str\n",
        "    authors: List[Person]\n",
        "    summaries: List[str] = []\n",
        "    editors: List[Person] = []\n",
        "    translators: List[Person] = []\n",
        "    subjects: List[str] = []\n",
        "    bookshelves: List[str] = []\n",
        "    languages: List[str] = []\n",
        "    copyright: Optional[bool] = None\n",
        "    media_type: Optional[str] = None\n",
        "    formats: Dict[str, HttpUrl]\n",
        "    download_count: int\n",
        "\n",
        "\n",
        "class BookMetadataOutput(BaseModel):\n",
        "    title: str\n",
        "    authors: List[Person]\n",
        "    summaries: List[str] = []\n",
        "    subjects: List[str] = []\n",
        "    characters: List[str] = []\n",
        "    languages: List[str] = []\n",
        "    copyright: Optional[bool] = None\n",
        "    media_type: Optional[str] = None\n",
        "    text_url: HttpUrl\n",
        "\n",
        "\n",
        "class CharacterLLMInput(BaseModel):\n",
        "    book: BookMetadataOutput\n",
        "    character_name: str\n",
        "    snippets: List[Dict[int, str]]"
      ],
      "metadata": {
        "id": "PKMRg1i9FKwq"
      },
      "outputs": [],
      "execution_count": 101
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definição de funções auxiliares"
      ],
      "metadata": {
        "id": "Tid0HNCOFXxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _normalize_name(name: str) -> str:\n",
        "    name = name.strip().strip('\"').strip(\"'\").strip()\n",
        "    name = re.sub(r\"[’'´`]+s$\", \"\", name)\n",
        "    return name\n",
        "\n",
        "\n",
        "def _name_tokens(name: str) -> Set[str]:\n",
        "    return {t for t in re.split(r\"\\s+\", name) if t}\n",
        "\n",
        "\n",
        "def _author_token_set(authors) -> Set[str]:\n",
        "    tokens = set()\n",
        "    for a in authors:\n",
        "        norm = _normalize_name(a.name.replace(\",\", \" \"))\n",
        "        tokens.update(_name_tokens(norm))\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def _is_stop_candidate(token_text: str) -> bool:\n",
        "    if not token_text:\n",
        "        return True\n",
        "    if token_text.lower() in EN_STOPWORDS:\n",
        "        return True\n",
        "    if len(token_text) <= 2:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _strip_accents(s: str) -> str:\n",
        "    nfkd = unicodedata.normalize(\"NFD\", s)\n",
        "    return \"\".join(ch for ch in nfkd if unicodedata.category(ch) != \"Mn\")\n",
        "\n",
        "\n",
        "def _split_paragraphs(text: str) -> List[str]:\n",
        "    raw_paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
        "    return [p.strip() for p in raw_paragraphs if p.strip()]\n",
        "\n",
        "\n",
        "def extract_book_metadata(book: int) -> BookMetadataInput:\n",
        "    response = requests.get(f\"https://gutendex.com/books/{book}\")\n",
        "    response.raise_for_status()\n",
        "\n",
        "    gutendex = BookMetadataInput.model_validate(response.json())\n",
        "    return gutendex\n",
        "\n",
        "\n",
        "def extract_character_names(book) -> List[str]:\n",
        "    text = \" \".join(book.summaries or [])\n",
        "    doc = nlp(text)\n",
        "\n",
        "    author_tokens = _author_token_set(book.authors)\n",
        "    raw_candidates: Set[str] = set()\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ != \"PERSON\":\n",
        "            continue\n",
        "\n",
        "        name = _normalize_name(ent.text)\n",
        "        if not name:\n",
        "            continue\n",
        "\n",
        "        tokens = _name_tokens(name)\n",
        "        if not tokens:\n",
        "            continue\n",
        "\n",
        "        if tokens.issubset(author_tokens):\n",
        "            continue\n",
        "\n",
        "        raw_candidates.add(name)\n",
        "\n",
        "    return sorted(raw_candidates)\n",
        "\n",
        "\n",
        "def enrich_books_with_characters(book: BookMetadataInput) -> BookMetadataOutput:\n",
        "    return BookMetadataOutput(\n",
        "        authors=book.authors,\n",
        "        title=book.title,\n",
        "        summaries=book.summaries,\n",
        "        subjects=book.subjects,\n",
        "        characters=extract_character_names(book),\n",
        "        text_url=book.formats['text/plain; charset=us-ascii']\n",
        "    )\n",
        "\n",
        "\n",
        "def find_relevant_paragraphs(\n",
        "        paragraphs: List[str],\n",
        "        character_name: str,\n",
        "        window: int = 1,\n",
        "        min_length: int = 80,\n",
        ") -> List[str]:\n",
        "    name_norm = _strip_accents(character_name).lower()\n",
        "    pattern = re.compile(rf\"\\b{re.escape(name_norm)}\\b\")\n",
        "\n",
        "    hit_indices: List[int] = []\n",
        "    for i, p in enumerate(paragraphs):\n",
        "        p_norm = _strip_accents(p).lower()\n",
        "        if pattern.search(p_norm):\n",
        "            hit_indices.append(i)\n",
        "\n",
        "    if not hit_indices:\n",
        "        return []\n",
        "\n",
        "    selected_indices: List[int] = []\n",
        "    seen: Set[int] = set()\n",
        "\n",
        "    for idx in hit_indices:\n",
        "        start = max(0, idx - window)\n",
        "        end = min(len(paragraphs), idx + window + 1)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if j not in seen:\n",
        "                seen.add(j)\n",
        "                selected_indices.append(j)\n",
        "\n",
        "    selected_indices.sort()\n",
        "    return [\n",
        "        {idx: paragraphs[i].replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\")}\n",
        "        for idx, p in enumerate(selected_indices)\n",
        "        if len(paragraphs[i].strip()) >= min_length\n",
        "    ]\n",
        "\n",
        "\n",
        "def build_character_llm_input(\n",
        "        book_output: BookMetadataOutput,\n",
        "        character_name: str,\n",
        "        window: int = 1\n",
        ") -> CharacterLLMInput:\n",
        "    response = requests.get(book_output.text_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    paragraphs = _split_paragraphs(response.text)\n",
        "    snippets = find_relevant_paragraphs(\n",
        "        paragraphs,\n",
        "        character_name=character_name,\n",
        "        window=window\n",
        "    )\n",
        "\n",
        "    return CharacterLLMInput(\n",
        "        book=book_output,\n",
        "        character_name=character_name,\n",
        "        snippets=snippets\n",
        "    )\n",
        "\n",
        "\n",
        "def build_prompt_from_context(ctx: CharacterLLMInput) -> str:\n",
        "    max_snippets_in_prompt = 50\n",
        "\n",
        "    data = json.dumps({\n",
        "        \"metadados_livro\": {\n",
        "            \"titulo\": ctx.book.title,\n",
        "            \"autores\": [a.model_dump() for a in ctx.book.authors],\n",
        "            \"assuntos\": ctx.book.subjects,\n",
        "            \"personagens_detectados_no_resumo\": ctx.book.characters\n",
        "        },\n",
        "        \"personagem_alvo\": {\n",
        "            \"nome\": ctx.character_name\n",
        "        },\n",
        "        \"trechos_texto\": ctx.snippets[:max_snippets_in_prompt]\n",
        "    }, ensure_ascii=False)\n",
        "\n",
        "    print(data)\n",
        "    return data\n",
        "\n",
        "\n",
        "def process_book_characters(book: BookMetadataOutput) -> List[dict]:\n",
        "    results: List[dict] = []\n",
        "\n",
        "    for character_name in book.characters:\n",
        "        if not character_name.strip:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n\\nProcessando {character_name} para o livro {book.title}\")\n",
        "        ctx = build_character_llm_input(\n",
        "            book_output=book,\n",
        "            character_name=character_name,\n",
        "            window=1,\n",
        "        )\n",
        "\n",
        "        if not book.characters:\n",
        "            print(f\"Nenhum personagem encontrado para o livro {book.title}\")\n",
        "            continue\n",
        "\n",
        "        if not ctx.snippets:\n",
        "            print(f\"Nenhum parágrafo encontrado personagem de nome {character_name} encontrado para o livro {book.title}\")\n",
        "\n",
        "        prompt_text = build_prompt_from_context(ctx)\n",
        "        llm_output = trait_chain.invoke({\"texto\": prompt_text})\n",
        "        print(trait_prompt.template)\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"character_name\": character_name,\n",
        "                \"character_context\": ctx,\n",
        "                \"llm_output\": llm_output,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        build_row_from_result(ctx, llm_output)\n",
        "        sleep(1)\n",
        "\n",
        "        break\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def build_row_from_result(ctx: CharacterLLMInput, llm_output: str) -> dict:\n",
        "    book = ctx.book\n",
        "\n",
        "    authors_json = json.dumps([a.model_dump() for a in book.authors], ensure_ascii=False)\n",
        "    summaries_json = json.dumps(book.summaries, ensure_ascii=False)\n",
        "    subjects_json = json.dumps(book.subjects, ensure_ascii=False)\n",
        "    characters_json = json.dumps(book.characters, ensure_ascii=False)\n",
        "    snippets_json = json.dumps(ctx.snippets, ensure_ascii=False)\n",
        "\n",
        "    cleaned = re.sub(r\"^```(\\w+)?\", \"\", llm_output.strip())\n",
        "    cleaned = re.sub(r\"```$\", \"\", cleaned).strip()\n",
        "\n",
        "    print(json.loads(cleaned)[\"inferences\"])\n",
        "    print(json.loads(cleaned)[\"resume\"])\n",
        "\n",
        "    row = {\n",
        "        \"title\": book.title,\n",
        "        \"authors\": authors_json,\n",
        "        \"summaries\": summaries_json,\n",
        "        \"subjects\": subjects_json,\n",
        "        \"characters\": characters_json,\n",
        "        \"text_url\": book.text_url,\n",
        "        \"character_name\": ctx.character_name,\n",
        "        \"snippets\": snippets_json,\n",
        "        \"traits\": json.dumps(cleaned, ensure_ascii=False),\n",
        "    }\n",
        "\n",
        "    append_row_to_csv(row)\n",
        "    return row\n",
        "\n",
        "\n",
        "def append_row_to_csv(row: dict, path: str = \"characters_dataset.csv\") -> None:\n",
        "    file_exists = os.path.exists(path)\n",
        "\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "eTGKJyV3E7GD"
      },
      "outputs": [],
      "execution_count": 102
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuração para a pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "kbRJWnTjHzxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "print(f\"\\n\\nGOOGLE_API_KEY está vazia.\" if GOOGLE_API_KEY is None else \"\")\n",
        "\n",
        "llm_model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "trait_prompt = PromptTemplate(\n",
        "    input_variables=[\"texto\"],\n",
        "    template=(\"\"\"\n",
        "        Você é um analista especializado em perfis de personagens literários. Com base EXCLUSIVAMENTE nos trechos fornecidos abaixo, produza um JSON com inferências de personalidade. Regras obrigatórias:\n",
        "        - A saída deve conter apenas JSON. Não inclua explicações.\n",
        "        - O JSON final deve ter exatamente as chaves:\n",
        "        - \"inferences\": lista de objetos.\n",
        "        - \"resume\": um resumo com NO MÁXIMO 200 caracteres EM PORTUGUÊS BARSILEIRO sobre quem é o personagem com base EM TODOS OS TRECHOS enviado no contexto como \"trechos_texto\".\n",
        "        - Cada objeto dentro de \"inferences\" deve seguir esta estrutura:\n",
        "        - \"id\": o mesmo id recebido na entrada, correspondente ao trecho analisado.\n",
        "        - \"traits\": lista de adjetivos que descrevem a personalidade detectada NO TRECHO.\n",
        "        - Você deve retirar apenas traits claramente justificáveis pelo trecho.\n",
        "        - Se não houver traits relevantes, NÃO inclua esse item na lista \"inferences\".\n",
        "        - Todos os adjetivos devem ser:\n",
        "        - em PORTUGUÊS BRASILEIRO,\n",
        "        - no gênero masculino,\n",
        "        - escolhidos APENAS desta lista:\n",
        "        astuto, inteligente, rabugento, sensível, misterioso, impulsivo, determinado, melancólico, orgulhoso, cínico, ingênuo, corajoso, leal, ambicioso, manipulador, compulsivo, vaidoso, inseguro, ciumento, observador, tímido, protetor, teimoso, irônico, frio, calculista, curioso, sonhador, dramático, solitário, pessimista, temperamental, independente, romântico, carismático, arrogante, gentil, questionador, sarcástico, bondoso, rigoroso, rebelde, racional, emocional, aventureiro, prudente, ético, manipulável, resiliente, confuso, audacioso, obediente, altruísta, egoísta, controlador, precavido, trabalhador, preguiçoso, perfeccionista, desorganizado, criativo, pragmático, rígido, metódico, procrastinador, desconfiado, dominador, generoso, esperançoso, focado, inovador, conservador, inconsequente, responsável, ansioso, mediador, inflexível, empático, exigente, submisso, autoritário.\n",
        "        Contexto: \\n {texto}\n",
        "    \"\"\"),\n",
        ")\n",
        "\n",
        "trait_chain = trait_prompt | llm_model | StrOutputParser()"
      ],
      "metadata": {
        "id": "GPqhOGl_H6QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e393353b-bf8d-40c0-b3f0-d5128e37da6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 103
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definição da pipeline"
      ],
      "metadata": {
        "id": "6nIUIVD3F41_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_metadata_runnable = RunnableLambda(\n",
        "    lambda inp: extract_book_metadata(inp[\"book_id\"])\n",
        ")\n",
        "\n",
        "enrich_with_characters_runnable = RunnableLambda(\n",
        "    lambda book: enrich_books_with_characters(book)\n",
        ")\n",
        "\n",
        "book_metadata_pipeline = fetch_metadata_runnable | enrich_with_characters_runnable\n",
        "full_book_pipeline = book_metadata_pipeline | RunnableLambda(process_book_characters)"
      ],
      "metadata": {
        "id": "EKrfHTNrHncn"
      },
      "outputs": [],
      "execution_count": 104
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Execução da pipeline"
      ],
      "metadata": {
        "id": "WhfG2h_kIKzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    path = 'livros_ptbr.csv'\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        book_ids = df['id'][2]\n",
        "    else:\n",
        "        print(\"Arquivo não encontrado, usando um ID de teste.\")\n",
        "        book_ids = [69187]\n",
        "\n",
        "    for book_id in book_ids:\n",
        "        pipeline_input = {\"book_id\": book_id}\n",
        "        results = full_book_pipeline.invoke(pipeline_input)\n",
        "\n",
        "        for index, item in enumerate(results):\n",
        "            ctx = item[\"character_context\"]\n",
        "            llm_output = item[\"llm_output\"]\n",
        "\n",
        "            print(f\"\\n\\nResultado {index + 1}:\")\n",
        "            print(\" Personagens Encontrados:\", ctx.book.characters)\n",
        "            print(\" Livro:\", ctx.book.title)\n",
        "            print(\" Personagem:\", ctx.character_name)\n",
        "            print(\" Trechos relevantes:\", len(ctx.snippets))\n",
        "            print(\" LLM:\", llm_output)\n",
        "\n",
        "            sleep(5)"
      ],
      "metadata": {
        "id": "yHkUId8her8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f43a2e-3428-4f9c-fb97-6718164c1ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo não encontrado, usando um ID de teste.\n",
            "\n",
            "\n",
            "Processando Bertoleza para o livro O Cortiço\n",
            "{\"metadados_livro\": {\"titulo\": \"O Cortiço\", \"autores\": [{\"name\": \"Azevedo, Aluísio\", \"birth_year\": 1857, \"death_year\": 1913}], \"assuntos\": [\"Brazil -- Social life and customs -- 19th century -- Fiction\", \"Historical fiction\", \"Immigrants -- Brazil -- Fiction\", \"Landlords -- Fiction\", \"Man-woman relationships -- Fiction\", \"Slums -- Fiction\"], \"personagens_detectados_no_resumo\": [\"Bertoleza\", \"João\", \"João Romão\", \"Miranda\", \"Romão\"]}, \"personagem_alvo\": {\"nome\": \"Bertoleza\"}, \"trechos_texto\": [{\"0\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"1\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"2\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"3\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"4\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"5\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"6\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"7\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"8\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"9\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"10\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"11\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"12\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"13\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"14\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"15\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"16\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"17\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"18\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"19\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"20\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"21\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"22\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"23\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"24\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"25\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"26\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"27\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"28\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"29\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"30\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"31\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"32\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"33\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"34\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"35\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"36\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"37\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"38\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"39\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"40\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"41\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"42\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"43\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"44\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"45\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"46\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"47\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"48\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}, {\"49\": \"This website includes information about Project Gutenberg™,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\"}]}\n",
            "\n",
            "        Você é um analista especializado em perfis de personagens literários. Com base EXCLUSIVAMENTE nos trechos fornecidos abaixo, produza um JSON com inferências de personalidade. Regras obrigatórias:\n",
            "        - A saída deve conter apenas JSON. Não inclua explicações.\n",
            "        - O JSON final deve ter exatamente as chaves:\n",
            "        - \"inferences\": lista de objetos.\n",
            "        - \"resume\": um resumo com NO MÁXIMO 200 caracteres EM PORTUGUÊS BARSILEIRO sobre quem é o personagem com base EM TODOS OS TRECHOS enviado no contexto como \"trechos_texto\".\n",
            "        - Cada objeto dentro de \"inferences\" deve seguir esta estrutura:\n",
            "        - \"id\": o mesmo id recebido na entrada, correspondente ao trecho analisado.\n",
            "        - \"traits\": lista de adjetivos que descrevem a personalidade detectada NO TRECHO.\n",
            "        - Você deve retirar apenas traits claramente justificáveis pelo trecho.\n",
            "        - Se não houver traits relevantes, NÃO inclua esse item na lista \"inferences\".\n",
            "        - Todos os adjetivos devem ser:\n",
            "        - em PORTUGUÊS BRASILEIRO,\n",
            "        - no gênero masculino,\n",
            "        - escolhidos APENAS desta lista:\n",
            "        astuto, inteligente, rabugento, sensível, misterioso, impulsivo, determinado, melancólico, orgulhoso, cínico, ingênuo, corajoso, leal, ambicioso, manipulador, compulsivo, vaidoso, inseguro, ciumento, observador, tímido, protetor, teimoso, irônico, frio, calculista, curioso, sonhador, dramático, solitário, pessimista, temperamental, independente, romântico, carismático, arrogante, gentil, questionador, sarcástico, bondoso, rigoroso, rebelde, racional, emocional, aventureiro, prudente, ético, manipulável, resiliente, confuso, audacioso, obediente, altruísta, egoísta, controlador, precavido, trabalhador, preguiçoso, perfeccionista, desorganizado, criativo, pragmático, rígido, metódico, procrastinador, desconfiado, dominador, generoso, esperançoso, focado, inovador, conservador, inconsequente, responsável, ansioso, mediador, inflexível, empático, exigente, submisso, autoritário.\n",
            "        Contexto: \n",
            " {texto}\n",
            "    \n",
            "[]\n",
            "Os trechos fornecidos não contêm informações sobre a personagem Bertoleza, impossibilitando a inferência de sua personalidade ou a criação de um resumo sobre ela.\n",
            "\n",
            "\n",
            "Resultado 1:\n",
            " Personagens Encontrados: ['Bertoleza', 'João', 'João Romão', 'Miranda', 'Romão']\n",
            " Livro: O Cortiço\n",
            " Personagem: Bertoleza\n",
            " Trechos relevantes: 128\n",
            " LLM: ```json\n",
            "{\n",
            "  \"inferences\": [],\n",
            "  \"resume\": \"Os trechos fornecidos não contêm informações sobre a personagem Bertoleza, impossibilitando a inferência de sua personalidade ou a criação de um resumo sobre ela.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "execution_count": 105
    }
  ]
}